<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Stanford&#39;s Machine Learning Course: Week 2</title>
	
	
	<link rel="stylesheet" href="https://domfleischmann.github.io/blog/css/style.css">
	
	<meta name="generator" content="Hugo 0.26" />
</head>
<body>
	<header>
		  <h1><a href="https://domfleischmann.github.io/blog">Dominik&#39;s Blog</a></h1>
	</header>

	<main>
		<article>
			<h1>Stanford&#39;s Machine Learning Course: Week 2</h1>
			<time>17.05.2017 13:02</time>
			<div>
				<p><img src="https://imgs.xkcd.com/comics/machine_learning.png" alt="Machine Learning">
 <span id="imagecap"><a href="https://xkcd.com/1838/">https://xkcd.com/1838/</a></span></p>

<p>So on monday I finished the second week of <a href="https://www.coursera.org/learn/machine-learning">Cousera&rsquo;s Machine Learning course by Stanford University</a>. First it started with some basic steps for installing Octave/Matlab, I obviously chose Octave over Matlab as it&rsquo;s free software. After that there were some videos about Multivariate Linear Regression, it&rsquo;s basically the same as linear regression so their wasn&rsquo;t much difficulty, although in the final video Andrew started talking about polynomial regression and how we could use it for fitting better our models. I wished he would&rsquo;ve explained a little more that part as it seemed quite important for real applications, but maybe he will elaborate more in some other week. He also explained feature scaling and learning rates.</p>

<p>The next session was about Normal Equation, a very useful substitute of Gradient Descent if you  have few features (less than 10.000, I know right, nearly no features at all). The great part about it is that you don&rsquo;t have to iterate many times like in gradient descent, you just calculate it once. After that there was the quiz and this time I failed a  question about feature scaling, it was a question about calculating the exact value of a feature after applying feature scaling on it. I guess I didn&rsquo;t quite understand the question.</p>

<p>But then came the most tedious part of the week, the Octave tutorial, I like learning new programming languages but I usually do it by trying stuff and consulting documentation on the go. But as I wanted to do the course like it was intended I first watched all the videos about the basic syntax and functionalities (76 minutes!). After finally finshing that the most exiting part of the week arrived, the programming assignment. Althoug there wasn&rsquo;t a lot to do it got me pretty hyped for the coming weeks and after doing the graded tasks I just had to continue with the optional ones.</p>

<p>All in all enjoyed this week just as much as the last one. Octave seems like a quite interesting tool, it is quite intuitive applying the mathematical formulas nearly directly because of the high level nature of the language. I think it was a good joice on Andrew Ng&rsquo;s part using it over some other language. Now we will see what awaits us in week 3.</p>

			</div>
			<div>
				<ul id="tags">
					
				</ul>
			</div>
			<div>
				
			</div>
		</article>
	</main>
	<footer>
		<p>&copy; 2017 <a href="https://domfleischmann.github.io">Dominik Fleischmann</a>  | <a href="https://github.com/DomFleischmann">My Github </a></p>
	</footer>
</body>
</html>
 
